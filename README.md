# Agente de Reinforcement Learning con HMM para Trading del ETF EPU (PerÃº)

## ğŸ“Œ DescripciÃ³n general

Este proyecto implementa un **agente de Reinforcement Learning (RL)** que integra un  
**Hidden Markov Model (HMM)** para identificar **regÃ­menes ocultos del mercado** y tomar
decisiones de **compra, venta o mantener (Buy / Sell / Hold)** sobre el activo financiero:

> **EPU â€“ iShares MSCI Peru ETF**

El sistema combina informaciÃ³n proveniente de **mÃºltiples fuentes heterogÃ©neas**:
- Precios histÃ³ricos del ETF
- Indicadores macroeconÃ³micos del PerÃº
- Volatilidad histÃ³rica (archivo Excel)
- Noticias financieras con anÃ¡lisis de sentimiento

El objetivo principal es **evaluar si la incorporaciÃ³n de estados latentes (HMM) mejora
el desempeÃ±o de un agente de RL**, comparÃ¡ndolo contra un agente que no utiliza dicha
informaciÃ³n.

---

## ğŸ¯ Objetivos

### Objetivo general
Construir y evaluar un agente de trading basado en **Reinforcement Learning** que utilice
un **Hidden Markov Model** para enriquecer la representaciÃ³n del estado del mercado.

### Objetivos especÃ­ficos
- Integrar al menos **4 fuentes de datos distintas** (APIs y archivos Excel).
- Modelar **estados ocultos del mercado** (alcista, bajista y lateral) mediante HMM.
- Entrenar un agente **Deep Q-Network (DQN)** con y sin informaciÃ³n del HMM.
- Comparar el desempeÃ±o usando mÃ©tricas financieras estÃ¡ndar.

---

## ğŸ§  Arquitectura del sistema

Fuentes de Datos
â”‚
â”œâ”€â”€ Precios (Yahoo Finance - EPU)
â”œâ”€â”€ Macro PerÃº (BCRPData)
â”œâ”€â”€ Volatilidad (Excel)
â””â”€â”€ Noticias y Sentimiento (Alpha Vantage)
â†“
Limpieza y Feature Engineering
â†“
Hidden Markov Model (RegÃ­menes de mercado)
â†“
Estado aumentado (features + probabilidades HMM)
â†“
Agente de Reinforcement Learning (DQN)
â†“
Decisiones: Buy / Sell / Hold


---

## ğŸ“Š Fuentes de datos

| Tipo | Fuente | Uso |
|----|------|----|
| Precios | Yahoo Finance (`EPU`) | Retornos y dinÃ¡mica del mercado |
| Macro PerÃº | BCRPData (API) | Contexto macroeconÃ³mico |
| Volatilidad | Archivo Excel | Medida de riesgo |
| Noticias | Alpha Vantage â€“ Market News & Sentiment | AnÃ¡lisis de sentimiento |

---

## ğŸ§© Componentes principales

### 1. Ingesta y limpieza de datos
- Descarga automÃ¡tica de precios y noticias vÃ­a API.
- Lectura de archivos Excel.
- NormalizaciÃ³n de fechas y escalas.
- Manejo de valores faltantes mediante *forward fill*.

### 2. Hidden Markov Model (HMM)
- Implementado con `GaussianHMM`.
- IdentificaciÃ³n de regÃ­menes de mercado.
- Uso de **probabilidades posteriores** como parte del estado del agente.

### 3. Reinforcement Learning
- Agente **Deep Q-Network (DQN)**.
- Espacio de acciones discreto: `Hold`, `Buy`, `Sell`.
- FunciÃ³n de recompensa basada en retorno diario neto de costos de transacciÃ³n.

### 4. EvaluaciÃ³n
- **Cumulative Return**
- **Sharpe Ratio**
- **Maximum Drawdown**
- ComparaciÃ³n entre:
  - DQN **con HMM**
  - DQN **sin HMM**

---

## ğŸ“ Estructura del proyecto

Agente-de-Reinforcement-Learning/
â”œâ”€ src/ # CÃ³digo fuente
â”‚ â”œâ”€ data/ # Ingesta de datos
â”‚ â”œâ”€ features/ # Feature engineering
â”‚ â”œâ”€ hmm/ # Modelos HMM
â”‚ â”œâ”€ rl/ # Entorno y entrenamiento RL
â”‚ â””â”€ utils/ # Cache y mÃ©tricas
â”œâ”€ scripts/
â”‚ â””â”€ run_pipeline.py # EjecuciÃ³n completa del flujo
â”œâ”€ notebooks/ # AnÃ¡lisis exploratorio
â”œâ”€ data/ # Datos (NO versionados)
â”œâ”€ artifacts/ # Modelos entrenados (NO versionados)
â”œâ”€ requirements.txt
â”œâ”€ .gitignore
â””â”€ README.md

## âš™ï¸ InstalaciÃ³n y entorno

### 1. Crear entorno virtual

```bash
python -m venv .venv
source .venv/bin/activate      # Linux / macOS
# .\.venv\Scripts\activate     # Windows
```
### 2. Instalar dependencias

pip install -r requirements.txt

### 3. Configurar API Key

Crear un archivo .env en la raÃ­z del proyecto:

ALPHAVANTAGE_API_KEY=TU_API_KEY_AQUI

La API Key se obtiene gratuitamente en:
https://www.alphavantage.co/support/#api-key

## â–¶ï¸ EjecuciÃ³n del pipeline

```bash
python scripts/run_pipeline.py
```

Este script realiza las siguientes etapas:

1. Descarga o reutiliza datos cacheados
2. Entrena el modelo **Hidden Markov Model (HMM)**
3. Entrena el agente de **Reinforcement Learning (DQN)**
4. EvalÃºa y compara los resultados obtenidos

---

## ğŸ—ƒï¸ Cache de noticias (optimizaciÃ³n)

Para evitar exceder los lÃ­mites de la API de **Alpha Vantage**, el sistema implementa un mecanismo de cache local:

- Las respuestas de la API se **almacenan en disco**.
- Si un rango de fechas ya fue consultado, **no se realiza un nuevo request**.
- El cache estÃ¡ indexado por:
  - Ticker
  - Rango temporal
  - Topics
  - ParÃ¡metros de la API

Este enfoque mejora la **eficiencia**, **reproducibilidad** y **estabilidad** del experimento.

---

## âš ï¸ Consideraciones importantes

- Este proyecto tiene **fines estrictamente acadÃ©micos**.
- No constituye una recomendaciÃ³n de inversiÃ³n.
- No se consideran fricciones reales del mercado como:
  - *Slippage*
  - Liquidez
  - Restricciones regulatorias

---

## ğŸ“š TecnologÃ­as utilizadas

- Python 3.11
- Pandas / NumPy
- Scikit-learn
- hmmlearn
- Gymnasium
- Stable-Baselines3 (DQN)
- Alpha Vantage API
- Yahoo Finance

---

## ğŸ“ˆ Posibles extensiones

- Uso de PPO o SAC en lugar de DQN
- Incorporar costos de transacciÃ³n dinÃ¡micos
- ValidaciÃ³n *walk-forward*
- Uso de LSTM o Transformers en el agente RL
- Trading multi-activo

---

## ğŸ‘¤ Autor

Proyecto desarrollado con fines acadÃ©micos para el estudio de:

**Reinforcement Learning, Hidden Markov Models y Finanzas Computacionales**  
aplicados al anÃ¡lisis del mercado peruano.
